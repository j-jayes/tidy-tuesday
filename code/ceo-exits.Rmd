---
title: "ceo-exits"
author: "JJayes"
date: "27/04/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(rvest)
library(glue)

```

### Reading in data

```{r}

departures <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-27/departures.csv')
```

### EDA

```{r}
departures %>% 
  skimr::skim()


departures %>% 
  filter(!is.na(departure_code)) %>% 
  count(fyear, departure_code) %>% 
  ggplot(aes(fyear, n, fill = as.factor(departure_code))) +
  geom_col() +
  labs(fill = NULL)
```


Relationship between tenure and probability of dying in office?

Departure code 5 means voluntary retirement. What words are associated with this?

```{r}
library(tidytext)

departures %>% 
  filter(departure_code == 5) %>% 
  unnest_tokens(word, notes) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = T)
  

```

What about predicting whether the person was fired or retired based on sec fillings? 

- this would entail some webscraping - yay
- then a bit of NLP
- can see if using only verbs and adjectives makes it easier to distinguish between the two.
- use step smote to see if we can get around the class imbalance.

```{r}
departures %>% 
  filter(!is.na(departure_code),
         !is.na(eight_ks)) %>% 
  count(fyear, departure_code) %>% 
  ggplot(aes(fyear, n, fill = as.factor(departure_code))) +
  geom_col() +
  labs(fill = NULL)

departures %>% 
  filter(!is.na(departure_code),
         !is.na(eight_ks)) %>% 
  count(departure_code, sort = T)

departures %>% select(tenure_no_ceodb)

```

Can do 5s vs 3s (and 4s)? Retired vs involuntarily dismissed for poor performance?

```{r}

df <- departures %>% 
  filter(!is.na(eight_ks),
         departure_code %in% c(3,4,5))

```

```{r}

df <- df %>% 
  # select(eight_ks) %>% 
  mutate(sec_url = strsplit(eight_ks, "\r")) %>% 
  unnest(sec_url)

```

Some of the links have way too much in the url

### Scraper

```{r}
library(httr)

get_sec_text <- function(sec_url){
  
  message(glue("Getting review content from {sec_url}"))
  
  html <- GET(sec_url, add_headers('user-agent' = 'SEC data scraper ([[j0nathanjayes@gmail.com]])'))
  
  text <- html %>%
    read_html() %>% 
    html_text() %>% 
    str_squish()
  
}

df_text <- df %>% 
  # filter(row_number() < 10) %>%
  mutate(sec_filling = map(sec_url, possibly(get_sec_text, "failed")))

df_text %>% 
  unnest(sec_filling) %>% view()

write_rds(df_text, "data/ceo-exits/df_text.rds")

```

## Scrape reasons

Get the reasons for departure from tidytuesday description

```{r}
url <- "https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-04-27/readme.md"

departure_type <- read_html(url) %>% 
  html_nodes("table") %>%
  html_table()

departure_type <- departure_type[[2]]

write_rds(departure_type, "data/ceo-exits/description.rds")

```

---------------------------------------------------------------------------

### Using the data

```{r}

df <- read_rds("data/ceo-exits/df_text.rds")

df <- df %>% 
  unnest(sec_filling)

df <- df %>% 
  mutate(sec_filling = str_to_lower(sec_filling))

df <- df %>%
  mutate(n_char = nchar(sec_filling)) %>% 
  filter(n_char < 10000) %>% 
  mutate(sec_filling = str_remove_all(sec_filling, "--"),
         sec_filling = str_remove_all(sec_filling, "=="),
         sec_filling = str_remove_all(sec_filling, "__"))

df <- df %>% 
  mutate(sec_filling = str_remove_all(sec_filling, "([0-9][0-9][0-9]+)"))

```

### Feature engineering: tenure

```{r}
df_tenure <- df %>%
  mutate(
    still_there = lubridate::dmy(still_there), 
    interim = case_when(
      tolower(interim_coceo) == "interim" ~ "Interim CEO",
      TRUE ~ "Not interim"
    )) %>% 
  select(coname, gvkey, fyear, still_there, interim) %>%
  distinct() %>% 
  group_by(gvkey) %>% 
  arrange(gvkey, fyear) %>%
  mutate(tenure = case_when(
    is.na(still_there) ~ -1*(fyear - lead(fyear, 1)),
    TRUE ~ lubridate::year(still_there) - fyear
  )) %>%
  ungroup()

df_tenure <- df_tenure %>% 
  select(coname:fyear, tenure) %>% 
  mutate(key = glue("{coname}-{gvkey}-{fyear}")) %>% 
  select(key, tenure)

df <- df %>% 
  mutate(key = glue("{coname}-{gvkey}-{fyear}"))

df <- df %>% 
  left_join(df_tenure, by = "key") %>% 
  select(-key)

```

I need to make a decision about whether to widen the data

```{r}
df <- df %>%
  mutate(departure = case_when(
    
    departure_code == 5 ~ "Retired",
    departure_code %in% c(3,4) ~ "Fired"
  )) %>% 
  mutate(departure = as.factor(departure))

```

Just for now take only the outcome and predictor

```{r}
# df <- df %>% 
#   select(departure, sec_filling)

```

### EDA

```{r}
df %>% 
  count(departure, sort = T)

```

### A bit more feaure engineering

- Idea is to use the words that are most specific to each document with the bind_tf_idf command and then filter to include the words that are the sec_filling column. 
- We only want the text after "Item 5"

maybe do a str_detect "item 5" 

```{r}

df <- df %>% 
  # head(1000) %>% 
  mutate(sec_filling_code = case_when(
    
    str_detect(sec_filling, "item 5") ~ "item 5",
    str_detect(sec_filling, "item 9") ~ "item 9",
    str_detect(sec_filling, "item 12") ~ "item 12",
    str_detect(sec_filling, "failed") ~ "failed",
    TRUE ~ "Other"
  )) %>% 
  mutate(sec_filling_text = case_when(
    
    str_detect(sec_filling_code, "Other") ~ sec_filling,
    TRUE ~ str_remove(sec_filling, glue(".*{sec_filling_code}"))
    
  )) %>% 
  select(-sec_filling)

df %>% 
  count(sec_filling_code, sort = T)

df <- df %>% 
  filter(sec_filling_code != "failed")


```

```{r}
library(tidytext)

df_words <- df %>% 
  unnest_tokens(word, "sec_filling_text", token = "words") %>% 
  anti_join(stop_words)

```

### Bag of words approach

```{r}

# write_rds(df_words, "data/ceo-exits/df_word.rds", compress = "gz")

df_words %>% 
  count(departure, word) %>% 
  group_by(departure) %>% 
  slice_max(n, n = 15) %>% 
  ungroup() %>% 
  mutate(word = reorder_within(word, n, departure)) %>% 
  ggplot(aes(n, word, fill = departure)) +
  geom_col() +
  facet_wrap(~ departure, scales = "free_y") +
  scale_y_reordered()

```

Tidy log odds package

```{r}
library(tidylo)

df_log_odds <- df_words %>% 
  count(departure, word) %>% 
  bind_log_odds(departure, word, n) %>% 
  filter(n > 200) %>% 
  group_by(departure) %>% 
  slice_max(log_odds_weighted, n = 20) %>% 
  ungroup() %>% 
  arrange(departure, - log_odds_weighted)

df_log_odds %>% 
  mutate(word = reorder_within(word, log_odds_weighted, departure)) %>% 
  ggplot(aes(log_odds_weighted, word, fill = departure)) +
  geom_col() +
  facet_wrap(~ departure, scales = "free_y") +
  scale_y_reordered()

```

So we can limit the number of words to be only those in the tidy log odds 

```{r}

df_words_to_keep <- df_words %>% 
  count(departure, word) %>% 
  bind_log_odds(departure, word, n) %>% 
  filter(n > 175)

df_words_to_keep <- df_words_to_keep %>% 
  select(word)

df_words_to_keep <- df_words_to_keep %>% 
  mutate(index = glue("a{row_number()}")) %>% 
  pivot_wider(names_from = index, values_from = word) %>% 
  as.character()


df$distinct_words <- str_extract_all(df$sec_filling_text, paste(df_words_to_keep, collapse = '|')) %>% 
  sapply(., paste, collapse = ", ")

write_rds(df, "data/ceo-exits/df_words.rds", compress = "gz")

```


# Modelling

### Initial split

```{r}
df_short <- df %>% 
  select(departure, distinct_words, tenure)

set.seed(123)
tidy_split <- initial_split(df_short, strata = departure)
tidy_train <- training(tidy_split)

tidy_folds <- vfold_cv(tidy_train)

tidy_train %>% 
  skimr::skim()
```


### Make recipe

```{r}
library(textrecipes)
library(themis)

ceo_rec <- recipe(departure ~ .,  
                  data = tidy_train) %>% 
  step_impute_median(tenure) %>% 
  step_tokenize(distinct_words) %>% 
  step_tokenfilter(distinct_words, max_tokens = 500) %>% 
  step_tfidf(distinct_words) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  # for upsampling of the minotiry class
  step_smote(departure)


```

### Model spec

```{r}
svm_spec <- svm_linear() %>% 
  set_mode("classification") %>% 
  set_engine("LiblineaR")

```

### Model workflow

```{r}
svm_wf <- workflow() %>% 
  add_recipe(ceo_rec) %>% 
  add_model(svm_spec)
```

### Fit resamples

Nb should include the metric set in here.

```{r}
library(doParallel)
library(parallel)

no_cores <- detectCores(logical = TRUE) 

cl <- makeCluster(no_cores-2)  
registerDoParallel(cl)  

set.seed(123)

svm_res <- fit_resamples(
  svm_wf,
  tidy_folds,
  metrics = metric_set(accuracy, recall, precision),
  control = control_resamples(save_pred = T)
)

write_rds(svm_res, "data/ceo-exits/model_1.rds", compress = "gz")
```

### What are the results like?

```{r}
collect_metrics(svm_res)

```

### Visualizing results

Here we show a confusion matrix of the 

```{r}
svm_res %>% 
  conf_mat_resampled() %>% 
  autoplot.conf_mat(object, type = "mosaic")

```

### Final fitted

Fit the best model to the until-now unseen testing data.

```{r}

final_fitted <- last_fit(
  svm_wf,
  tidy_split,
  metrics = metric_set(accuracy, recall, precision)
)

write_rds(final_fitted, "data/ceo-exits/model_fitted.rds", compress = "gz")
```

Then draw conf_mat and just publish.

```{r}
final_fitted %>% collect_metrics()
```

